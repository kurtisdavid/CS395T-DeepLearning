{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "They only mean center so we found the mean pixel value of faces and normalize with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.Grayscale(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5089547997389491],\n",
    "                                     std=[1])])\n",
    "allImages = datasets.ImageFolder(root='./training',transform = data_transform)\n",
    "label_mapping = torch.FloatTensor([float(clazz) for clazz in allImages.classes])\n",
    "# label_mappin\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it output from [0,1] rather than [1900,2010]\n",
    "label_mapping_scaled = (label_mapping - label_mapping.min())/(label_mapping.max() - label_mapping.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(allImages,batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers, final_output):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.conv_params = {'kernel_size': 3, 'padding': 1}\n",
    "        self.width = 186\n",
    "        self.height = 171\n",
    "        self.layer_dict = {\n",
    "            18: [2,2,2,2],\n",
    "            34: [3,4,6,3],\n",
    "            50: [3,4,6,3],\n",
    "            101: [3,4,23,3]\n",
    "        }\n",
    "        self.bottleneck = True if n_layers in [50,101] else False\n",
    "        self.layers = {}\n",
    "        \n",
    "        in_channels = 1\n",
    "        out_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 7, stride = 2, padding = 3),\n",
    "            nn.BatchNorm2d(num_features = out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1, dilation = 1)\n",
    "        )\n",
    "        self.width = self.width / 2\n",
    "        self.width = math.floor( (self.width - 1) / 2 + 1 )\n",
    "        self.height = self.height / 2\n",
    "        self.height = math.floor( (self.height - 1) / 2 + 1)\n",
    "        print(\"Height is now:\", self.height, \"Width is now:\", self.width)\n",
    "        \n",
    "        \n",
    "        in_channels = 64\n",
    "        \n",
    "        num_repeat = self.layer_dict[n_layers]\n",
    "        for i in range(2,6):\n",
    "            self.res_layer = i\n",
    "            # [ [blocks], transition ]\n",
    "            self.layers[self.res_layer] = [[], None]\n",
    "            for j in range(num_repeat[i-2]):\n",
    "                self.create_block(in_channels, out_channels, j)\n",
    "                if j == 0:\n",
    "                    self.add_transition(in_channels, out_channels)\n",
    "                if self.bottleneck:\n",
    "                    in_channels = out_channels * 4\n",
    "                else:\n",
    "                    in_channels = out_channels\n",
    "            out_channels = out_channels * 2\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # global average pooling\n",
    "        self.global_avg = nn.AvgPool2d(kernel_size = (self.width,self.height), stride = 1)\n",
    "        # fully connected to final\n",
    "        self.output = nn.Linear(in_channels,1)\n",
    "        \n",
    "    def create_block(self, in_channels, out_channels, block_num):\n",
    "        if self.bottleneck:\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, stride = 1, kernel_size = 1),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, stride = 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.Conv2d(out_channels, out_channels*4, stride = 2 if (block_num == 0 and self.res_layer > 2) else 1, kernel_size = 1),\n",
    "                nn.BatchNorm2d(num_features = out_channels*4),\n",
    "            )\n",
    "            print(\"Added\", \"conv_bottleneck\" + str(self.res_layer) + \"_\" + str(block_num), \"input: \" + str(in_channels), \"output: \" + str(out_channels*4))\n",
    "        else:\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, stride = 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, stride = 2 if block_num == 0 else 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels)\n",
    "            )\n",
    "            print(\"Added\", \"conv\" + str(self.res_layer) + \"_\" + str(block_num), \"input: \" + str(in_channels), \"output: \" + str(out_channels))\n",
    "        self.add_module(\"conv\" + str(self.res_layer) + \"_\" + str(block_num), block)\n",
    "        self.layers[self.res_layer][0].append(block)\n",
    "        \n",
    "        if block_num == 0 and self.res_layer > 2:\n",
    "            self.height = math.floor( (self.height - 1) / 2 + 1)\n",
    "            self.width = math.floor( (self.width - 1) / 2 + 1)\n",
    "            print(\"Height is now:\", self.height, \"Width is now:\", self.width)\n",
    "        \n",
    "    def add_transition(self, in_channel, out_channel):\n",
    "        transition = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel * 4, stride = 2 if self.res_layer > 2 else 1, kernel_size = 1),\n",
    "            nn.BatchNorm2d(num_features = out_channel*4),\n",
    "        )\n",
    "#         transition = nn.AvgPool2d(kernel_size = 3, stride = 2 if self.res_layer > 2 else 1, padding = 1)\n",
    "        self.add_module(\"transition\"+ str(self.res_layer), transition)\n",
    "        self.layers[self.res_layer][1] = transition\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # go through conv1\n",
    "        X = self.conv1(X)\n",
    "        # go through residuals\n",
    "        for i in range(2,self.res_layer + 1):\n",
    "            layers,transition = self.layers[i]\n",
    "            for j,layer in enumerate(layers):\n",
    "                identity = X\n",
    "                if j == 0:\n",
    "                    identity = transition(X)\n",
    "                X = layer(X) + identity\n",
    "                X = self.relu(X)\n",
    "        X = self.global_avg(X)\n",
    "        X = X.view(X.shape[0],-1)\n",
    "        X = self.output(X)\n",
    "        print(X.shape)\n",
    "        return X.view(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXt(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers, final_output):\n",
    "        super(ResNeXt,self).__init__()\n",
    "        self.conv_params = {'kernel_size': 3, 'padding': 1}\n",
    "        self.width = 186\n",
    "        self.height = 171\n",
    "        self.layer_dict = {\n",
    "            50: [3,4,6,3],\n",
    "            101: [3,4,23,3]\n",
    "        }\n",
    "        self.bottleneck = True if n_layers in [50,101] else False\n",
    "        self.layers = {}\n",
    "        self.C = 32\n",
    "        \n",
    "        in_channels = 1\n",
    "        out_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 7, stride = 2, padding = 3),\n",
    "            nn.BatchNorm2d(num_features = out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1, dilation = 1)\n",
    "        )\n",
    "        self.width = self.width / 2\n",
    "        self.width = math.floor( (self.width - 1) / 2 + 1 )\n",
    "        self.height = self.height / 2\n",
    "        self.height = math.floor( (self.height - 1) / 2 + 1)\n",
    "        print(\"Height is now:\", self.height, \"Width is now:\", self.width)\n",
    "        \n",
    "        \n",
    "        in_channels = 64\n",
    "        out_channels = 128\n",
    "        \n",
    "        num_repeat = self.layer_dict[n_layers]\n",
    "        for i in range(2,6):\n",
    "            self.res_layer = i\n",
    "            # [ [blocks], transition ]\n",
    "            self.layers[self.res_layer] = [[], None]\n",
    "            for j in range(num_repeat[i-2]):\n",
    "                blocks = []\n",
    "                for k in range(self.C):\n",
    "                    blocks.append(self.create_block(in_channels, out_channels//self.C, j, k))\n",
    "                if j == 0:\n",
    "                    self.add_transition(in_channels, out_channels)\n",
    "                in_channels = out_channels * 2\n",
    "                self.layers[self.res_layer][0].append(blocks)\n",
    "            out_channels = out_channels * 2\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # global average pooling\n",
    "        print(self.width,self.height)\n",
    "        self.global_avg = nn.AvgPool2d(kernel_size = (self.width,self.height), stride = 1)\n",
    "        # fully connected to final\n",
    "        self.output = nn.Linear(in_channels,1)\n",
    "        \n",
    "    def create_block(self, in_channels, out_channels, block_num, k):\n",
    "        if self.bottleneck:\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, stride = 1, kernel_size = 1),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, stride = 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.Conv2d(out_channels, out_channels*self.C*2, stride = 2 if (block_num == 0 and self.res_layer > 2) else 1, kernel_size = 1),\n",
    "                nn.BatchNorm2d(num_features = out_channels*self.C*2),\n",
    "            )\n",
    "            print(\"Added\", \"conv_bottleneck\" + str(self.res_layer) + \"_\" + str(block_num) + \"_\" + str(k), \"input: \" + str(in_channels), \"output: \" + str(out_channels*self.C*2))\n",
    "        else:\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, stride = 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, stride = 2 if block_num == 0 else 1, **self.conv_params),\n",
    "                nn.BatchNorm2d(num_features = out_channels)\n",
    "            )\n",
    "            print(\"Added\", \"conv\" + str(self.res_layer) + \"_\" + str(block_num) + \"_\" + str(k), \"input: \" + str(in_channels), \"output: \" + str(out_channels))\n",
    "        self.add_module(\"conv\" + str(self.res_layer) + \"_\" + str(block_num) + \"_\" + str(k), block)\n",
    "        \n",
    "        if block_num == 0 and self.res_layer > 2 and k == 0:\n",
    "            self.height = math.floor( (self.height - 1) / 2 + 1)\n",
    "            self.width = math.floor( (self.width - 1) / 2 + 1)\n",
    "            print(\"Height is now:\", self.height, \"Width is now:\", self.width)\n",
    "        \n",
    "        return block\n",
    "        \n",
    "    def add_transition(self, in_channel, out_channel):\n",
    "        transition = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel * 2, stride = 2 if self.res_layer > 2 else 1, kernel_size = 1),\n",
    "            nn.BatchNorm2d(num_features = out_channel*2),\n",
    "        )\n",
    "#         transition = nn.AvgPool2d(kernel_size = 3, stride = 2 if self.res_layer > 2 else 1, padding = 1)\n",
    "        self.add_module(\"transition\"+ str(self.res_layer), transition)\n",
    "        self.layers[self.res_layer][1] = transition\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # go through conv1\n",
    "        X = self.conv1(X)\n",
    "        # go through residuals\n",
    "        for i in range(2,self.res_layer + 1):\n",
    "            layers,transition = self.layers[i]\n",
    "            for j,block in enumerate(layers):\n",
    "                identity = X\n",
    "                res = None\n",
    "                for k,layer in enumerate(block):\n",
    "                    if res is None:\n",
    "                        res = layer(X)\n",
    "                    else:\n",
    "                        res = res + layer(X)\n",
    "                if j == 0:\n",
    "                    identity = transition(X)\n",
    "                X = res + identity\n",
    "                X = self.relu(X)\n",
    "        X = self.global_avg(X)\n",
    "        X = X.view(X.shape[0],-1)\n",
    "        X = self.output(X)\n",
    "        return X.view(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height is now: 43 Width is now: 47\n",
      "Added conv_bottleneck2_0_0 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_1 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_2 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_3 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_4 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_5 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_6 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_7 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_8 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_9 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_10 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_11 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_12 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_13 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_14 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_15 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_16 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_17 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_18 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_19 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_20 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_21 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_22 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_23 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_24 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_25 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_26 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_27 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_28 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_29 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_30 input: 64 output: 256\n",
      "Added conv_bottleneck2_0_31 input: 64 output: 256\n",
      "Added conv_bottleneck2_1_0 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_1 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_2 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_3 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_4 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_5 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_6 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_7 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_8 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_9 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_10 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_11 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_12 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_13 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_14 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_15 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_16 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_17 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_18 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_19 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_20 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_21 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_22 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_23 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_24 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_25 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_26 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_27 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_28 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_29 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_30 input: 256 output: 256\n",
      "Added conv_bottleneck2_1_31 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_0 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_1 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_2 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_3 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_4 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_5 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_6 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_7 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_8 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_9 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_10 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_11 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_12 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_13 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_14 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_15 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_16 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_17 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_18 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_19 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_20 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_21 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_22 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_23 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_24 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_25 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_26 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_27 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_28 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_29 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_30 input: 256 output: 256\n",
      "Added conv_bottleneck2_2_31 input: 256 output: 256\n",
      "Added conv_bottleneck3_0_0 input: 256 output: 512\n",
      "Height is now: 22 Width is now: 24\n",
      "Added conv_bottleneck3_0_1 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_2 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_3 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_4 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_5 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_6 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_7 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_8 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_9 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_10 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_11 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_12 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_13 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_14 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_15 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_16 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_17 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_18 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_19 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_20 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_21 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_22 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_23 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_24 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_25 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_26 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_27 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_28 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_29 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_30 input: 256 output: 512\n",
      "Added conv_bottleneck3_0_31 input: 256 output: 512\n",
      "Added conv_bottleneck3_1_0 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_1 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_2 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_3 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_4 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_5 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_6 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_7 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_8 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_9 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_10 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_11 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_12 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_13 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_14 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_15 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_16 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_17 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_18 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_19 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_20 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_21 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_22 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_23 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_24 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_25 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_26 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_27 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_28 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_29 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_30 input: 512 output: 512\n",
      "Added conv_bottleneck3_1_31 input: 512 output: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added conv_bottleneck3_2_0 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_1 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_2 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_3 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_4 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_5 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_6 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_7 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_8 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_9 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_10 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_11 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_12 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_13 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_14 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_15 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_16 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_17 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_18 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_19 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_20 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_21 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_22 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_23 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_24 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_25 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_26 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_27 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_28 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_29 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_30 input: 512 output: 512\n",
      "Added conv_bottleneck3_2_31 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_0 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_1 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_2 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_3 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_4 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_5 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_6 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_7 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_8 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_9 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_10 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_11 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_12 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_13 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_14 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_15 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_16 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_17 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_18 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_19 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_20 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_21 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_22 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_23 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_24 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_25 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_26 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_27 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_28 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_29 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_30 input: 512 output: 512\n",
      "Added conv_bottleneck3_3_31 input: 512 output: 512\n",
      "Added conv_bottleneck4_0_0 input: 512 output: 1024\n",
      "Height is now: 11 Width is now: 12\n",
      "Added conv_bottleneck4_0_1 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_2 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_3 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_4 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_5 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_6 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_7 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_8 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_9 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_10 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_11 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_12 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_13 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_14 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_15 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_16 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_17 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_18 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_19 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_20 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_21 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_22 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_23 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_24 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_25 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_26 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_27 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_28 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_29 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_30 input: 512 output: 1024\n",
      "Added conv_bottleneck4_0_31 input: 512 output: 1024\n",
      "Added conv_bottleneck4_1_0 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_1 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_2 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_3 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_4 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_5 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_6 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_7 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_8 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_9 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_10 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_11 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_12 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_13 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_14 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_15 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_16 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_17 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_18 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_19 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_20 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_21 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_22 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_23 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_24 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_25 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_26 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_27 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_28 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_29 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_30 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_1_31 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_0 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_1 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_2 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_3 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_4 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_5 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_6 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_7 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_8 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_9 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_10 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_11 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_12 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_13 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_14 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_15 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_16 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_17 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_18 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_19 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_20 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_21 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_22 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_23 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_24 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_25 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_26 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_27 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_28 input: 1024 output: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added conv_bottleneck4_2_29 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_30 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_2_31 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_0 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_1 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_2 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_3 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_4 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_5 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_6 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_7 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_8 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_9 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_10 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_11 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_12 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_13 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_14 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_15 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_16 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_17 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_18 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_19 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_20 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_21 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_22 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_23 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_24 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_25 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_26 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_27 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_28 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_29 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_30 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_3_31 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_0 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_1 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_2 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_3 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_4 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_5 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_6 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_7 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_8 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_9 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_10 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_11 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_12 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_13 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_14 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_15 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_16 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_17 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_18 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_19 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_20 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_21 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_22 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_23 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_24 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_25 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_26 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_27 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_28 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_29 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_30 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_4_31 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_0 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_1 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_2 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_3 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_4 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_5 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_6 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_7 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_8 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_9 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_10 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_11 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_12 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_13 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_14 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_15 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_16 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_17 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_18 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_19 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_20 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_21 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_22 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_23 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_24 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_25 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_26 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_27 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_28 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_29 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_30 input: 1024 output: 1024\n",
      "Added conv_bottleneck4_5_31 input: 1024 output: 1024\n",
      "Added conv_bottleneck5_0_0 input: 1024 output: 2048\n",
      "Height is now: 6 Width is now: 6\n",
      "Added conv_bottleneck5_0_1 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_2 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_3 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_4 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_5 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_6 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_7 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_8 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_9 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_10 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_11 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_12 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_13 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_14 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_15 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_16 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_17 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_18 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_19 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_20 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_21 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_22 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_23 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_24 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_25 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_26 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_27 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_28 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_29 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_30 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_0_31 input: 1024 output: 2048\n",
      "Added conv_bottleneck5_1_0 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_1 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_2 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_3 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_4 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_5 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_6 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_7 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_8 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_9 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_10 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_11 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_12 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_13 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_14 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_15 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_16 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_17 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_18 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_19 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_20 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_21 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_22 input: 2048 output: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added conv_bottleneck5_1_23 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_24 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_25 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_26 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_27 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_28 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_29 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_30 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_1_31 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_0 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_1 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_2 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_3 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_4 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_5 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_6 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_7 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_8 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_9 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_10 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_11 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_12 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_13 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_14 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_15 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_16 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_17 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_18 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_19 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_20 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_21 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_22 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_23 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_24 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_25 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_26 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_27 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_28 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_29 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_30 input: 2048 output: 2048\n",
      "Added conv_bottleneck5_2_31 input: 2048 output: 2048\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNeXt(50,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24414465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-45968cd65981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_mapping_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d1660cb396d1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    106\u001b[0m                         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(resnet.parameters(),lr = 0.001, betas = (0.9,0.999))\n",
    "loss_metric = nn.L1Loss()\n",
    "n_epochs = 100\n",
    "iteration = 0\n",
    "for e in range(n_epochs):\n",
    "    losses = []\n",
    "    for batch_input, batch_labels in dataloader:\n",
    "        if iteration % 1 == 0:\n",
    "            print(iteration)\n",
    "        # make sure to zero out gradient\n",
    "        resnet.zero_grad()\n",
    "        \n",
    "        # move to gpu + get correct labels\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_labels = label_mapping_scaled[batch_labels].to(device)\n",
    "        \n",
    "        loss = loss_metric(resnet(batch_input),batch_labels)\n",
    "        losses.append(loss.data)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        iteration += 1\n",
    "        del batch_input\n",
    "        del batch_labels\n",
    "#         break\n",
    "    print(\"Epoch %d: Training Loss: %0.3f\" % (e,np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.Grayscale(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5089547997389491],\n",
    "                                     std=[1])])\n",
    "valImages = datasets.ImageFolder(root='./validation',transform = data_transform)\n",
    "label_mapping_v = torch.FloatTensor([float(clazz) for clazz in valImages.classes])\n",
    "\n",
    "# label_mapping\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1905.,  1906.,  1908.,  1909.,  1910.,  1911.,  1912.,  1913.,\n",
       "         1914.,  1915.,  1916.,  1919.,  1922.,  1923.,  1924.,  1925.,\n",
       "         1926.,  1927.,  1928.,  1929.,  1930.,  1931.,  1932.,  1933.,\n",
       "         1934.,  1935.,  1936.,  1937.,  1938.,  1939.,  1940.,  1941.,\n",
       "         1942.,  1943.,  1944.,  1945.,  1946.,  1947.,  1948.,  1949.,\n",
       "         1950.,  1951.,  1952.,  1953.,  1954.,  1955.,  1956.,  1957.,\n",
       "         1958.,  1959.,  1960.,  1961.,  1962.,  1963.,  1964.,  1965.,\n",
       "         1966.,  1967.,  1968.,  1969.,  1970.,  1971.,  1972.,  1973.,\n",
       "         1974.,  1975.,  1976.,  1977.,  1978.,  1979.,  1980.,  1981.,\n",
       "         1982.,  1983.,  1984.,  1985.,  1986.,  1987.,  1988.,  1989.,\n",
       "         1990.,  1991.,  1992.,  1993.,  1994.,  1995.,  1996.,  1997.,\n",
       "         1998.,  1999.,  2000.,  2001.,  2002.,  2003.,  2004.,  2005.,\n",
       "         2006.,  2007.,  2008.,  2009.,  2010.,  2011.,  2012.,  2013.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1933.,  1935.,  1936.,  1940.,  1944.,  1945.,  1946.,  1947.,\n",
       "         1949.,  1950.,  1951.,  1952.,  1954.,  1955.,  1959.,  1961.,\n",
       "         1962.,  1963.,  1965.,  1967.,  1968.,  1970.,  1972.,  1973.,\n",
       "         1975.,  1976.,  1977.,  1978.,  1979.,  1981.,  1983.,  1984.,\n",
       "         1990.,  1991.,  1992.,  2000.,  2001.,  2002.,  2005.,  2008.,\n",
       "         2011.,  2012.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping_scaled_v = (label_mapping_v - label_mapping.min())/(label_mapping.max() - label_mapping.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0253,  0.0380,  0.0886,  0.1392,  0.1519,  0.1646,\n",
       "         0.1772,  0.2025,  0.2152,  0.2278,  0.2405,  0.2658,  0.2785,\n",
       "         0.3291,  0.3544,  0.3671,  0.3797,  0.4051,  0.4304,  0.4430,\n",
       "         0.4684,  0.4937,  0.5063,  0.5316,  0.5443,  0.5570,  0.5696,\n",
       "         0.5823,  0.6076,  0.6329,  0.6456,  0.7215,  0.7342,  0.7468,\n",
       "         0.8481,  0.8608,  0.8734,  0.9114,  0.9494,  0.9873,  1.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping_scaled_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataloader = torch.utils.data.DataLoader(valImages,batch_size = 128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046557505\n"
     ]
    }
   ],
   "source": [
    "# to turn off running averages in batch norm\n",
    "resnet34.eval()\n",
    "losses = []\n",
    "for batch_input,batch_labels in valDataloader:\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_labels = label_mapping_scaled_v[batch_labels].to(device)\n",
    "    res = resnet34(batch_input)\n",
    "#     print(res)\n",
    "#     print(batch_labels)\n",
    "    loss = loss_metric(res,batch_labels)\n",
    "#     print(loss.data)\n",
    "    losses.append(loss.data)\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.1226, device='cuda:0'), tensor(0.1452, device='cuda:0'), tensor(0.1358, device='cuda:0'), tensor(0.1300, device='cuda:0'), tensor(0.1321, device='cuda:0'), tensor(0.1246, device='cuda:0'), tensor(0.1419, device='cuda:0'), tensor(0.1292, device='cuda:0'), tensor(0.1356, device='cuda:0'), tensor(0.1464, device='cuda:0'), tensor(0.1410, device='cuda:0'), tensor(0.1308, device='cuda:0'), tensor(0.1389, device='cuda:0'), tensor(0.1282, device='cuda:0'), tensor(0.1357, device='cuda:0'), tensor(0.1299, device='cuda:0'), tensor(0.1335, device='cuda:0'), tensor(0.1398, device='cuda:0'), tensor(0.1365, device='cuda:0'), tensor(0.1391, device='cuda:0'), tensor(0.1293, device='cuda:0'), tensor(0.1508, device='cuda:0'), tensor(0.1500, device='cuda:0'), tensor(0.1600, device='cuda:0'), tensor(0.1442, device='cuda:0'), tensor(0.1385, device='cuda:0'), tensor(0.1298, device='cuda:0'), tensor(0.1410, device='cuda:0'), tensor(0.1505, device='cuda:0'), tensor(0.1444, device='cuda:0'), tensor(0.1373, device='cuda:0'), tensor(0.1384, device='cuda:0'), tensor(0.1460, device='cuda:0'), tensor(0.1304, device='cuda:0'), tensor(0.1338, device='cuda:0'), tensor(0.1299, device='cuda:0'), tensor(0.1264, device='cuda:0'), tensor(0.1430, device='cuda:0'), tensor(0.1366, device='cuda:0'), tensor(0.1380, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2785,  0.8608,  0.8481,  0.7468,  0.3291,  0.2152,  0.4304,\n",
       "         0.3797,  0.5823,  0.1392,  0.7468,  0.5443,  0.0380,  0.2785,\n",
       "         0.4684,  0.4430,  0.6076], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
